{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349d57e3",
   "metadata": {},
   "source": [
    "# 1. 문자 단위 RNN(Char RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7aa0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418bc0b",
   "metadata": {},
   "source": [
    "####  [apple을 입력받으면 pple!를 출력하는 RNN을 구현]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1c73b",
   "metadata": {},
   "source": [
    "- **훈련 데이터 전처리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2192598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c709de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "# 입력의 크기는 문자 집합의 크기\n",
    "input_size = vocab_size \n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 문자에 고유한 정수 인덱스 부여\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) \n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1196b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 다시 문자 시퀀스로 보기위해서 반대로 정수로부터 문자를 얻음\n",
    "index_to_char={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b68ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data) # a, p, p, l, e에 해당\n",
    "print(y_data) # p, p, l, e, !에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79097922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 배치 차원 추가(파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받음.)\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348404d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# 입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줌\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f04c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터를 텐서로 바꿔줌\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "# 각 텐서의 크기를 확인\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807cce2",
   "metadata": {},
   "source": [
    "- **모델 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a9e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 모델을 구현\n",
    "# fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        # RNN 셀 구현\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        # 출력층 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) \n",
    "    \n",
    "    # 구현한 RNN 셀과 출력층을 연결\n",
    "    def forward(self, x): \n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7298b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 클래스로 정의한 모델을 net에 저장\n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "# 모델에 입력을 넣어서 출력의 크기를 확인\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7a022f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.5619158744812012 prediction:  [[3 0 3 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  l!l!!\n",
      "1 loss:  1.3192371129989624 prediction:  [[4 4 4 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppep\n",
      "2 loss:  1.0482114553451538 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
      "3 loss:  0.7714241743087769 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "4 loss:  0.5238173604011536 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "5 loss:  0.34458836913108826 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.22296974062919617 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.1414474993944168 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.09043611586093903 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.059101272374391556 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.03969884663820267 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.027485936880111694 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.019617266952991486 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.014411315321922302 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.010878214612603188 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.008428279310464859 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.006700689438730478 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.005465502385050058 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.004568507894873619 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.003900565905496478 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.003382598515599966 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.0029590961057692766 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.002596141304820776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.0022778857965022326 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.0019993477035313845 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.001759322709403932 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.0015558679588139057 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0013854411663487554 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.001243517268449068 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0011253503616899252 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.0010266418103128672 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0009436603868380189 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0008735773153603077 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.0008139432175084949 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.0007628788007423282 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0007187185692600906 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0006804871954955161 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.000647113483864814 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0006177405593916774 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0005918449023738503 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0005688309902325273 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0005482228589244187 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0005298061296343803 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0005131521029397845 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0004980940720997751 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0004843463539145887 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.00047174206702038646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.000460233713965863 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0004496306646615267 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0004397899901960045 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0004307593626435846 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0004223243740852922 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.00041448502452112734 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0004072174779139459 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.00040042647742666304 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.00039406438008882105 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0003881549637299031 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.00038260300061665475 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.00037745607551187277 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0003725712595041841 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.00036797241773456335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.00036373097100295126 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0003596563183236867 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0003558437747415155 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.00035217421827837825 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0003487190988380462 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0003454784455243498 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.00034235691418871284 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.00033935453393496573 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.00033649508259259164 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0003337309753987938 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.00033103831810876727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.00032848864793777466 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.00032593897776678205 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.00032343692146241665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0003210778522770852 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0003187187830917537 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0003163834917359054 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.00031411973759531975 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 loss:  0.00031187976128421724 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0003096874279435724 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0003075666318181902 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0003054696135222912 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0003033964312635362 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.000301394728012383 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.00029936915962025523 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.00029741512844339013 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0002954848459921777 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.00029357842868193984 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0002917196834459901 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0002898847160395235 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0002880736137740314 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.00028635779744945467 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0002846181741915643 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.0002829261648003012 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.00028116264729760587 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.00027954214601777494 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.00027787391445599496 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.00027627722010947764 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.00027465668972581625 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저와 손실 함수를 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "# 총 100번의 에포크를 학습\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) \n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result = outputs.data.numpy().argmax(axis=2) \n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc3a95",
   "metadata": {},
   "source": [
    "# 2. 문자 단위 RNN(Char RNN) - 더  많은 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd17852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c425059",
   "metadata": {},
   "source": [
    "- **훈련 데이터 전처리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0245706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0, 'k': 1, 'c': 2, \"'\": 3, 'w': 4, 'u': 5, 'l': 6, ',': 7, 'm': 8, 'b': 9, 'd': 10, '.': 11, 'n': 12, 'o': 13, 'y': 14, 'p': 15, 'h': 16, 'g': 17, 'r': 18, 't': 19, 'i': 20, 's': 21, 'e': 22, 'a': 23, ' ': 24}\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "# 중복을 제거한 문자 집합 생성\n",
    "char_set = list(set(sentence)) \n",
    "# 각 문자에 정수 인코딩\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "print(char_dic) # 공백도 여기서는 하나의 원소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d52ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "# 문자 집합의 크기를 확인\n",
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa60ece8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10  # 임의 숫자 지정\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5daa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 0, 24, 14, 13, 5, 24, 4, 23, 12]\n",
      "[0, 24, 14, 13, 5, 24, 4, 23, 12, 19]\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플의 입력 데이터와 레이블 데이터\n",
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b88a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n"
     ]
    }
   ],
   "source": [
    "# x 데이터 원-핫 인코딩\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "\n",
    "# 입력 데이터와 레이블 데이터를 텐서로 변환\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "# 훈련 데이터와 레이블 데이터의 크기를 확인\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f6edc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩 된 결과를 보기 위해서 첫번째 샘플만 출력\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dda2773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 24, 14, 13,  5, 24,  4, 23, 12, 19])\n"
     ]
    }
   ],
   "source": [
    "# 레이블 데이터의 첫번째 샘플도 출력\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ffd36",
   "metadata": {},
   "source": [
    "- **모델 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aeccb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    # 현재 hidden_size는 dic_size와 같음.\n",
    "    def __init__(self, input_dim, hidden_dim, layers): \n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d14ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "# 층을 두 개 쌓음\n",
    "net = Net(dic_size, hidden_size, 2) \n",
    "\n",
    "# 비용 함수와 옵티마이저를 선언\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "# 모델에 입력을 넣어서 출력의 크기를 확인\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서 <- (배치 차원 x 시점(timesteps) x 출력의 크기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb3b3972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 25])\n"
     ]
    }
   ],
   "source": [
    "# 정확도를 측정할 때는 이를 모두 펼쳐서 계산\n",
    "# 배치 차원과 시점 차원을 하나로 만듦\n",
    "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6e83b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeeeeeeeweeeeeeeweeeeeeeeeweeeeweeeeeeeeeweweeeeweeeeeeeweeeeeeeeeweeeeeeeeeeeeeeeeeeewweeeeeeweeeewwweeeeeeeeeweeweeeweeweeeeweeeeeweeeeeeeeeeeeeeeeweeeeewwweeeeeeweeeeeeeeeweeww\n",
      "ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "                                                                                                                                                                                   \n",
      "t tthttt rttrtttrtttttrrrrtrtttrtrtrnttrtrtrttrhrrhtrntttttttthrrtnrttrtrrtrrrtrtttrttttrttrtrrttrtttrtt rtrtthrtttrttrrtrttttrhrrtnrttrtrhrrtnrtttrttttttnrtrtrttttttttthttttrttrt\n",
      "tnttolto th tnttotnottntnnttttntnottnttootntnntoltn ttottnttntnotnnttnttntnnthtpnrttttn tnntn tnotnhnotnnttontnottnttontntnttsnhotnnttntnnnotnnltntoetntttn tnntnntttnnltnntoottott\n",
      "ontootootootootootootodootoooodtoooontooontoodmotoodtmotmntoo ootodtootntoodootoodoodontoodtottodootoooodloooooododtmodootooontootodtmolotoooooto oodootoootmoooodmooootoodoodtooto\n",
      "l  o onl  oo  lolo l loo ln   oo l o ln on olo  n ono    ol  oo o ool o o  o l  oooo o ol   loo o  ono   oloolo oolo oolooo   oo o oo  o o  o l  oolo  o  ono o oo  o oo o ooo  l  \n",
      "m       e              o             m                 em m           o m        m          h m                  h     mem       the              th   m m        n         h ms   \n",
      "       h  h          h           h             h  h   h t  h  h            he      h t     th          h   h           h   th  h  h               th                 hh  h  h      \n",
      "te te  h te o th  o t                            to  t  t th th        e    e to  th t     th    o               o  e t    th th th      to 'o    th      e  o        h th th   o t\n",
      "te t tt  toto to t  t t tt  e  tt    t    t n to to  t e  to to  e   to t  o  to tt     t  t    th t  te  eo tt  e  e toet to to t e  to to to e tt e to  e  tt       h th t ekto t\n",
      "teso ttor  to to t  tntott  to stoe  t t  t ttto to  t e  to te  em  to mteo  to stoe   t  t em to m  tns to st to'o  them to to them to te t to  toemto  er tto   soth tu toekt  t\n",
      "lusor tor  to 'o    dsto t, doe'todo l t et  ptorto  t e  the'os  s  to 'tto  ton'todn  tu thek to m  tns to 't tu'osochem torco them to 'o m tor toemto   s ttor d  th tusthekth t\n",
      "lnsor tor  to do    dsto t, don'todo l doed  lto to  l er toelo   s  do  lwod to dtodn  t  them torm  tns to  , tulod dhem todchethem to co d lor themtodo s  t   d  to tudtoemth t\n",
      "lnsor tor  eondo ln asth t, dondtodo l doep  i orto  t e  to co l s  do   wnd to dt en  ln them to    ln  do  , lodod toem toechethem to co d lor the todles  t  ed  l  on themwo t\n",
      "lnsoe to t eondo  n anthet, don't eo l doepo   o ton t e  tonco lost eo   wod tondt en  tn them to m  lns do  , tn od them toech them to co d lo  themtnd e   t   nc l  on themth t\n",
      "lnsoe to t to du  n anthet, eon't eo m aoepeo  o ton t es t nco enst eo   wndtwon't en  tn them to m  tnd wo  , tu od them t ech them to to d lo  thertndte s t   ns l  on thert lt\n",
      "mnsoektodt to bu  e anthet, eon't do m aoeteo  e ton t erstonbe  est eo   wn' won't dns tn them todm  tn' wo  , lu od them t ech ther to bo d bo  thers d e s tm  ns t  o  thems t \n",
      "m,soektort to tu ld anthet, eon't do m aoepeo  e ton ther to ce eect eo k and won't dns tn them tod   dnd wo k, bu oa ther toech ther to co ' to  thers d e s tm  ns ty on thers as\n",
      "m,sonktort to tu ld anthet, eon't do m apepeoi e ton ther to ch eect eo k and won't dns tn them tosk  dnd wo k, bu  d ther toech ther to to d bo  themsnd er  tm ens ty on thers as\n",
      "m,sonkwost to tuild anshit, don't do m apepeop e ton ther th ch eect do   and won't dns tn them tosk  dnd wo k, bu  dpthem thech ther to bo d bo  themsnd er  tm ens ty on thers as\n",
      "m,sonkwant to build anshil, don't doum dpepeop e ton ther to ch lect do   and won't dns in them tosk  dnd wo k, bu  dather toach ther tonbo d bor thersnd ess tm ens ty on thers as\n",
      "m,so kwesd to lu ld anship, don't doum do peop e ton them to fh lect to   and won't dns tn them tosks dnd wo k, bu  aather to sh them to lon' bor themsnd ess tm  ns ty tn thems as\n",
      "m,so  wesd to lu ld anship, don't doum ao peop e ton them to ch lect to   and won't dnsitn them tosks dnd wo k, bu  aathem toach them to lo ' bor themsnd ess tm  ns ty tn themshas\n",
      "mnso  went to bu ld anship, don't drum apepeople ton them te co lect wo   and aon't dssitn them tonks dnd wo k, bu  aather teach them to bon' bor themsnd ess tm ensity tn thems ac\n",
      "mnsor want to bu ld asship, don't doum apepeople to  ther to co lect wo   and won't dssitn them tonks dnd wo k, bu  aather teach them to bo g bor thersnd ess tmmensity of thems a \n",
      "mnson want to build asship, don't aoum up peop e tog ther to co lect ao   and won't acsitn them tonks dnd wo k, bu  aather toach ther to bong bor thersnd ess tmmensity of thers ac\n",
      "mnson want to bu ld anship, don't aoum up peoplo tog ther to co lect wo d and won't assitn them tonks dnd work, bu  aather teach ther to lon' bor thersnd ess tmmensity of thers ac\n",
      "mmson want to build anship, don't aoum up people together to co lect wood and won't assign them tonks and work, bu  aather teach them to long bor thersndless tmmensity of thers ac\n",
      "mmson want to luild anship, don't arum up people together to co lect wood and won't assign them tanks and work, bu  aather teach them to long for thersndless tmmensity of thers ac\n",
      "mmsou want to build anship, don't arum up people together to co lect wood wnd won't assign them tosks and work, bu  nather toach them to long for thertndless tmmensity of thers as\n",
      "mmsou want to build anship, don't drum up people together to collect wood and don't dssign them tosks and work, but dather teach them to long for themendless immensity of themseas\n",
      "tmsou want to build anship, don't drum up people together to collect wood and don't dssign them tosks and dork, but nather teach them to lo g for themendless immensity of therseas\n",
      "tmyou want to build anship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for themendless immensity of therseas\n",
      "tmyou want to build anship, don't drum up people together to collect wood and won't assiin them tosks and work, but rather teach them to long for therendless immensity of therseas\n",
      "tmyou want to build anship, don't arum up people together to collect wood and won't assiin them tasks and work, but rather toach them ta long for the endless immensity of themseas\n",
      "tmyou want to build anship, don't arum up people together to collect wood and don't assiin them tosks and work, but rather toach them to long for the endless immensity of themseas\n",
      "tmyou want to build anship, don't arum up people together te collect wood and won't assign them tanks and work, but rather teach them ta long for the endless immensity of the seac\n",
      "t you want to build anship, don't drum up people together te collect wood and don't dssign them tasks and dork, but rather teach them ta long for the endless immensity of the seac\n",
      "p you want to build anship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of themseas\n",
      "p you want to build anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "p sou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "m sou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "m sou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "m sou want to build a ship, don't drum up people together to collect wood and aon't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "p sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "p you want to build asship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # 옵티마이저 정의\n",
    "    optimizer.zero_grad()\n",
    "    # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
    "    outputs = net(X)\n",
    "    # 손실 함수를 정의\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a634ee8",
   "metadata": {},
   "source": [
    "# 3. 단어 단위 RNN - 임베딩 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285e795",
   "metadata": {},
   "source": [
    "#### ['Repeat is the best medicine for'을 입력받으면 'is the best medicine for memory'를 출력하는 RNN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78752f72",
   "metadata": {},
   "source": [
    "- **훈련 데이터 전처리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "334f92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89fe6f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'memory', 'Repeat', 'for', 'best', 'medicine', 'the']\n"
     ]
    }
   ],
   "source": [
    "# 임의의 문장\n",
    "sentence = \"Repeat is the best medicine for memory\".split()\n",
    "\n",
    "# 임의의 문장으로부터 단어장(vocabulary)을 만듦\n",
    "vocab = list(set(sentence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "167ede0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'memory': 2, 'Repeat': 3, 'for': 4, 'best': 5, 'medicine': 6, 'the': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "# 단어장의 단어에 고유한 정수 인덱스를 부여\n",
    "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
    "# 모르는 단어를 의미하는 UNK 토큰도 추가\n",
    "word2index['<unk>']=0\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55168f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# word2index에 단어를 입력하면 맵핑되는 정수를 리턴\n",
    "print(word2index['memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dbe0d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'is', 2: 'memory', 3: 'Repeat', 4: 'for', 5: 'best', 6: 'medicine', 7: 'the', 0: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "# 예측한 문장을 확인하기 위해 idx2word도 만듦\n",
    "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "print(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c55920c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 각 단어를 정수로 인코딩하는 동시에, 입력 데이터와 레이블 데이터를 만듦\n",
    "def build_data(sentence, word2index):\n",
    "    # 각 문자를 정수로 변환. \n",
    "    encoded = [word2index[token] for token in sentence]\n",
    "    # 입력 시퀀스와 레이블 시퀀스를 분리\n",
    "    input_seq, label_seq = encoded[:-1], encoded[1:] \n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
    "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
    "    return input_seq, label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4d4d820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 1, 7, 5, 6, 4]])\n",
      "tensor([[1, 7, 5, 6, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터\n",
    "X, Y = build_data(sentence, word2index)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8162b60",
   "metadata": {},
   "source": [
    "- **모델 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02b72e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 층을 추가\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
    "                                            embedding_dim=input_size)\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
    "                                batch_first=batch_first)\n",
    "        # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. 임베딩 층\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        output = self.embedding_layer(x)\n",
    "        # 2. RNN 층\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        # 3. 최종 출력층\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
    "        output = self.linear(output)\n",
    "        # 4. view를 통해서 배치 차원 제거\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
    "        return output.view(-1, output.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7637e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2029,  0.0398, -0.0294, -0.0904,  0.1750,  0.2717, -0.0284, -0.2550],\n",
      "        [ 0.1572,  0.0891, -0.1891, -0.1943,  0.0495,  0.3020, -0.1271, -0.3496],\n",
      "        [ 0.0551, -0.0904, -0.4286, -0.0910,  0.4636,  0.3458, -0.0625, -0.1969],\n",
      "        [ 0.2151, -0.0485, -0.1226, -0.1420,  0.2332,  0.3157,  0.0313, -0.0834],\n",
      "        [ 0.4322, -0.0489,  0.1390,  0.0090, -0.0422,  0.0390, -0.2230, -0.1841],\n",
      "        [ 0.3356,  0.0790,  0.1917,  0.0254, -0.1541,  0.0680, -0.2845, -0.4912]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터\n",
    "# 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
    "vocab_size = len(word2index)  \n",
    "# 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
    "input_size = 5  \n",
    "# RNN의 은닉층 크기\n",
    "hidden_size = 20  \n",
    "\n",
    "# 모델 생성\n",
    "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
    "# 손실함수 정의\n",
    "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태\n",
    "output = model(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74e2d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 예측값의 크기\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3d260ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치화된 데이터를 단어로 전환하는 함수\n",
    "decode = lambda y: [index2word.get(x) for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56480e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/201] 2.0722 \n",
      "Repeat best best for best <unk> <unk>\n",
      "\n",
      "[41/201] 1.5158 \n",
      "Repeat is best best medicine for memory\n",
      "\n",
      "[81/201] 0.8186 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[121/201] 0.3750 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[161/201] 0.1902 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[201/201] 0.1130 \n",
      "Repeat is the best medicine for memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "for step in range(201):\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 순방향 전파\n",
    "    output = model(X)\n",
    "    # 손실값 계산\n",
    "    loss = loss_function(output, Y.view(-1))\n",
    "    # 역방향 전파\n",
    "    loss.backward()\n",
    "    # 매개변수 업데이트\n",
    "    optimizer.step()\n",
    "    # 기록\n",
    "    if step % 40 == 0:\n",
    "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
    "        pred = output.softmax(-1).argmax(-1).tolist()\n",
    "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DH",
   "language": "python",
   "name": "dh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
